import scrapy
import json
import time
from scrapy.crawler import CrawlerProcess
import os


class HHSpider(scrapy.Spider):
    name = "hh_spider"

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.all_vacancies = []

    def start_requests(self):
        search_queries = ["Python", "–ü—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç", "–†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫"]
        for query in search_queries:
            for page in range(2):  # 2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –∑–∞–ø—Ä–æ—Å
                url = f"https://api.hh.ru/vacancies?text={query}&area=1&page={page}&per_page=20&only_with_salary=true"
                yield scrapy.Request(
                    url=url,
                    callback=self.parse,
                    meta={'query': query, 'page': page},
                    headers={'User-Agent': 'Mozilla/5.0'}
                )
                time.sleep(0.1)

    def parse(self, response):
        try:
            data = json.loads(response.text)
            vacancies = data.get('items', [])
            print(f"‚úì {response.meta['query']} - —Å—Ç—Ä. {response.meta['page'] + 1}: {len(vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π")

            for vacancy in vacancies:
                self.all_vacancies.append(vacancy)

        except Exception as e:
            print(f"‚úó –û—à–∏–±–∫–∞: {e}")

    def closed(self, reason):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ spider"""
        print(f"\n‚úÖ –°–æ–±—Ä–∞–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {len(self.all_vacancies)}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π JSON –º–∞—Å—Å–∏–≤
        with open("programming_vacancies.json", "w", encoding="utf-8") as f:
            json.dump(self.all_vacancies, f, ensure_ascii=False, indent=2)

        print("üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ programming_vacancies.json")


def run_parser():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–∞—Ä—Å–µ—Ä –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ"""
    process = CrawlerProcess(settings={
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'CONCURRENT_REQUESTS': 1,
        'DOWNLOAD_DELAY': 0.5,
        'LOG_LEVEL': 'INFO',
        'ROBOTSTXT_OBEY': False,
    })

    process.crawl(HHSpider)
    process.start()

    # –ß–∏—Ç–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    try:
        with open("programming_vacancies.json", "r", encoding="utf-8") as f:
            data = json.load(f)
            print(f"\nüìä –í—Å–µ–≥–æ –≤–∞–∫–∞–Ω—Å–∏–π –≤ —Ñ–∞–π–ª–µ: {len(data)}")
            return data
    except Exception as e:
        print(f"\n‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        return []


def run_parser_with_feed():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–∞—Ä—Å–µ—Ä —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º FEEDS (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ JSONL —Ñ–æ—Ä–º–∞—Ç–µ)"""
    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª, –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    if os.path.exists("programming_vacancies.json"):
        os.remove("programming_vacancies.json")

    process = CrawlerProcess(settings={
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'CONCURRENT_REQUESTS': 1,
        'DOWNLOAD_DELAY': 0.5,
        'LOG_LEVEL': 'INFO',
        'ROBOTSTXT_OBEY': False,
        'FEEDS': {
            'programming_vacancies.jsonl': {  # –ò—Å–ø–æ–ª—å–∑—É–µ–º .jsonl –¥–ª—è JSON Lines
                'format': 'jsonlines',
                'encoding': 'utf8',
                'overwrite': True
            }
        }
    })

    # –°–æ–∑–¥–∞–µ–º spider, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç yield'–∏—Ç—å —ç–ª–µ–º–µ–Ω—Ç—ã
    class HHYieldSpider(scrapy.Spider):
        name = "hh_yield_spider"

        def start_requests(self):
            search_queries = ["Python", "–ü—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç"]
            for query in search_queries:
                for page in range(2):
                    url = f"https://api.hh.ru/vacancies?text={query}&area=1&page={page}&per_page=20&only_with_salary=true"
                    yield scrapy.Request(
                        url=url,
                        callback=self.parse,
                        meta={'query': query, 'page': page},
                        headers={'User-Agent': 'Mozilla/5.0'}
                    )
                    time.sleep(0.1)

        def parse(self, response):
            try:
                data = json.loads(response.text)
                vacancies = data.get('items', [])
                print(f"‚úì {response.meta['query']} - —Å—Ç—Ä. {response.meta['page'] + 1}: {len(vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π")

                for vacancy in vacancies:
                    yield vacancy

            except Exception as e:
                print(f"‚úó –û—à–∏–±–∫–∞: {e}")

    process.crawl(HHYieldSpider)
    process.start()

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º JSONL –≤ –æ–±—ã—á–Ω—ã–π JSON
    return convert_jsonl_to_json()


def convert_jsonl_to_json():
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç JSONL —Ñ–∞–π–ª –≤ –æ–±—ã—á–Ω—ã–π JSON –º–∞—Å—Å–∏–≤"""
    jsonl_file = "programming_vacancies.jsonl"
    json_file = "programming_vacancies.json"

    if not os.path.exists(jsonl_file):
        print(f"‚ö† –§–∞–π–ª {jsonl_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return []

    data = []
    try:
        with open(jsonl_file, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                    data.append(json.loads(line))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π JSON
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"\n‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π –∏–∑ JSONL –≤ JSON")

        # –£–¥–∞–ª—è–µ–º JSONL —Ñ–∞–π–ª
        os.remove(jsonl_file)

        return data

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ JSONL: {e}")
        return []


def parse_jsonl_file(filename="programming_vacancies.jsonl"):
    """–ü–∞—Ä—Å–∏—Ç JSONL —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–∞–Ω–Ω—ã—Ö"""
    data = []
    if not os.path.exists(filename):
        return data

    try:
        with open(filename, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
        print(f"üìä –ü—Ä–æ—á–∏—Ç–∞–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {filename}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {filename}: {e}")

    return data


if __name__ == "__main__":
    print("=" * 60)
    print("SCRAPY –ü–ê–†–°–ï–† –î–õ–Ø HH.RU")
    print("=" * 60)

    print("\n1Ô∏è‚É£ –í–∞—Ä–∏–∞–Ω—Ç 1: –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)")
    vacancies1 = run_parser()

    print("\n2Ô∏è‚É£ –í–∞—Ä–∏–∞–Ω—Ç 2: –° –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º FEEDS (JSONL)")
    vacancies2 = run_parser_with_feed()

    print("\n" + "=" * 60)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print("=" * 60)
    print(f"–í–∞—Ä–∏–∞–Ω—Ç 1: {len(vacancies1)} –≤–∞–∫–∞–Ω—Å–∏–π")
    print(f"–í–∞—Ä–∏–∞–Ω—Ç 2: {len(vacancies2)} –≤–∞–∫–∞–Ω—Å–∏–π")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    all_vacancies = vacancies1 if len(vacancies1) > len(vacancies2) else vacancies2

    if all_vacancies:
        with open("hh_vacancies_final.json", "w", encoding="utf-8") as f:
            json.dump(all_vacancies, f, ensure_ascii=False, indent=2)
        print(f"\nüíæ –§–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ hh_vacancies_final.json")
        print(f"üìä –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π: {len(all_vacancies)}")
